# On Schedulers and Optimizers

References:
- [Visual guide to LR schedulers | Medium](https://freedium.cfd/https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)
- Why AdamW + OneCycleLR scheduler? TLDR: it seems that it is standard because of the hopes of achieveing superconvergence
    1. https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html
    2. https://residentmario.github.io/pytorch-training-performance-guide/lr-sched-and-optim.html
- [Finding good LR and the OneCycle policy](https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6)